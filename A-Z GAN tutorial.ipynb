{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generative Adversarial Networks (GANs) - An A-Z tutorial\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Background and Theory](#section2)\n",
    "3. [Setting up the Environment](#section3)\n",
    "4. [Dataset Introduction and Preparation](#section4)\n",
    "5. [Implementing a Basic GAN](#section5)\n",
    "6. [Training the GAN](#section6)\n",
    "7. [Limitations of GANs](#section7)\n",
    "8. [Summary and Conclusion](#section8)\n",
    "9. [Exercises and Challenges](#section9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## <a name='introduction'>Introduction</a>\n",
    "\n",
    "GANs are one of the most exciting advancements in machine learning in recent years. They have shown remarkable results in a variety of applications such as image synthesis, super-resolution, and image-to-image translation.\n",
    "\n",
    "This notebook aims to introduce you to the fundamental concepts of GANs, guide you through the necessary steps to grasp the GAN framework, and provide hands-on experience with the implementation of a GAN model. By the end of this guide, you will understand:\n",
    "\n",
    "- What GANs are and the theory behind them.\n",
    "- How to implement a basic GAN model.\n",
    "- How to train a GAN model.\n",
    "- The limitations and challenges of GANs.\n",
    "\n",
    "This guide is designed to be informative. It is to get you up to speed very quickly with how GANs work, not to challenge you to implement one yourself. This notebook covers the basics, but I encourage you to look at all the interesting follow-up work and see what is out there for yourself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Background and Theory <a class='anchor' id='section2'></a>\n",
    "\n",
    "GANs, short for Generative Adversarial Networks, are a class of machine learning frameworks introduced by [Ian Goodfellow and his colleagues in 2014](https://papers.nips.cc/paper_files/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html). They are designed to generate new, synthetic instances of data that can pass for real, existing instances.\n",
    "\n",
    "### 2.1 Fundamental Concepts\n",
    "\n",
    "GANs consist of two parts:\n",
    "\n",
    "**Generator:** This is the \"counterfeiter,\" tasked with creating fake data. The generator takes random noise as input and generates data (e.g., an image) as output.\n",
    "\n",
    "**Discriminator:** This is the \"police,\" trying to distinguish the fake data from real data. It is a binary classifier that outputs the probability that the given input comes from the real dataset rather than the generator.\n",
    "\n",
    "### 2.2 Architecture of GANs\n",
    "\n",
    "The generator and discriminator are set up to play a two-player minimax game, in which the generator tries to fool the discriminator, and the discriminator tries to not get fooled. This process can be summarized as follows:\n",
    "\n",
    "1. The generator creates a batch of fake data.\n",
    "2. The fake data, along with real data, are given to the discriminator.\n",
    "3. The discriminator classifies the data as real or fake.\n",
    "4. Both the generator and discriminator learn from their mistakes and adjust for the next round.\n",
    "\n",
    "This adversarial process leads the generator to produce increasingly realistic data, while the discriminator becomes better at distinguishing fake data. Over time, the generator can produce data that's almost indistinguishable from the real data.\n",
    "\n",
    "### 2.3 Understanding the Loss Function\n",
    "\n",
    "The loss function of a GAN reflects the adversarial relationship between the generator and the discriminator. The generator aims to minimize this function, while the discriminator aims to maximize it. This is the essence of the minimax game played between the two.\n",
    "\n",
    "Mathematically, the value function $V$ for a simple GAN can be written as:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]$$\n",
    "\n",
    "where:\n",
    "- `G` is the generator function, which maps from the latent space to the feature (data) space.\n",
    "- `D` is the discriminator function, which outputs the probability that the data came from the real dataset.\n",
    "- `z` is a random prior, a point in the latent space, which is the input that the generator uses to generate a fake sample.\n",
    "- `x` is a point in the feature (data) space.\n",
    "\n",
    "The first term in the equation is the expectation of the log-probability that the discriminator correctly classifies a real instance as real. The second term is the expectation of the log-probability that the discriminator correctly classifies a fake (generated) instance as fake.\n",
    "\n",
    "As the generator's goal is to fool the discriminator, it tries to maximize the probability that the discriminator incorrectly classifies a fake instance as real, hence it aims to minimize `log(1 - D(G(z)))`. On the other hand, the discriminator tries to correctly classify both real and fake instances, hence it aims to maximize both `log(D(x))` and `log(1 - D(G(z)))`.\n",
    "\n",
    "In practice, the generator minimizes the negative of this loss function, which is equivalent to maximizing the probability that the discriminator incorrectly classifies a fake instance as real.\n",
    "\n",
    "Through this process, both the generator and the discriminator improve their ability to generate realistic data and distinguish between real and fake data, respectively."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Setting up the Environment <a class='anchor' id='section3'></a>\n",
    "\n",
    "Now that you have a rough understanding of how GANs work, let's implement them. For the implementation, we need a couple of libraries."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (1.23.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: torch in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (0.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\320205369\\documents\\code\\envs\\base-env\\lib\\site-packages (from requests->torchvision) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy matplotlib torch torchvision tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all libraries successfully!\n"
     ]
    }
   ],
   "source": [
    "# Check if everything is downloaded correctly\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loaded all libraries successfully!\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Dataset Introduction and Preparation <a class='anchor' id='section4'></a>\n",
    "\n",
    "We will be using the MNIST dataset throughout this notebook. This dataset is easily accessible through the torchvision library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Download the MNIST dataset.\n",
    "mnist_dataset = datasets.MNIST(root='./.data/mnist_data/', train=True, download=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape:  torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x300 with 4 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The training data consists of 60.000 samples, each of 28 by 28 pixels.\n",
    "print(\"Dataset shape: \", mnist_dataset.data.shape)\n",
    "\n",
    "def plot_5(batch):\n",
    "    indices = np.random.choice(batch.shape[0], size=4, replace=False)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image = batch[idx]\n",
    "        axes[i].imshow(image, cmap='gray')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_5(mnist_dataset.data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 Preprocessing\n",
    "\n",
    "Before we can use the MNIST dataset for training our GAN, we need to preprocess the images. This preprocessing involves three main steps: casting the images to floats, normalizing the images and reshaping them into 1D tensors.\n",
    "\n",
    "**Casting:** By default, the MNIST data consists of integers. Before we can normalize the data, we need to turn the datatype into floats.\n",
    "\n",
    "**Normalization:** The pixel intensities in the MNIST images are originally in the range of 0 to 255. We normalize these intensities to be in the range [-1, 1]. Normalization helps to stabilize the training process and makes the output of the generator easier to interpret.\n",
    "\n",
    "**Reshaping:** Since we are going to use a simple GAN with multi-layer perceptrons, we need to flatten each image, which is originally a 2D tensor (28x28), into a 1D tensor (of size 784). This is because multi-layer perceptrons accept input data in a flat, vectorized form."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of batch:\n",
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        ...,\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [-1., -1., -1.,  ..., -1., -1., -1.]])\n",
      "\n",
      "Feature space size:\n",
      "784\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Normalize\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    # Casting\n",
    "    dataset = dataset.float()\n",
    "\n",
    "    # Normalization\n",
    "    dataset = Normalize(0.5, 0.5)(dataset)\n",
    "\n",
    "    # Reshaping\n",
    "    dataset = dataset.reshape(dataset.shape[0], -1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "preprocessed_dataset = preprocess_dataset(mnist_dataset.data)\n",
    "feature_dim = preprocessed_dataset.shape[-1]\n",
    "\n",
    "print(\"Example of batch:\")\n",
    "print(preprocessed_dataset)\n",
    "\n",
    "print(\"\\nFeature space size:\")\n",
    "print(feature_dim)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initializing a dataloader\n",
    "\n",
    "Now that the data is preprocessed, it can be loaded into a dataloader for easy retrieval. To initialize a dataloader, we will need to provide a batch size. For now, lets use a batch size of $128$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset=preprocessed_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Implementing a Basic GAN <a class='anchor' id='section5'></a>\n",
    "\n",
    "Having prepared our dataset, we're now ready to implement a basic GAN. In this section, we will define the generator and discriminator networks using multi-layer perceptrons. Each network will be implemented as a separate Python class using PyTorch, a popular deep learning library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1 Defining the Generator\n",
    "\n",
    "The generator network takes as input a random noise vector (latent vector) and transforms it into a 1D tensor that resembles our flattened MNIST images. The generator network typically includes several fully connected layers, and we'll use the ReLU activation function for all layers except for the last one, where we'll use the Tanh function to output values in the range [-1, 1], matching our normalized MNIST images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation for a generator network.\n",
    "\n",
    "    This generator implementation uses a standard multi-layer perceptron (MLP).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            output_size: int,\n",
    "            dropout: float,\n",
    "            leaky_relu_slope: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The initialization function for the generator.\n",
    "\n",
    "        :param input_size:          The input size of the generator.\n",
    "                                        This should be equal to the size of the random prior.\n",
    "        :param output_size:         The output size of the generator.\n",
    "                                        This should be equal to the size of the feature space.\n",
    "        :param dropout:             The dropout rate.\n",
    "        :param leaky_relu_slope:    The slope of the leaky ReLU activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the sequential multi-layer perceptron model using the given parameters.\n",
    "        self.sequential_model = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=256),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=512, out_features=1024),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=output_size),\n",
    "            # Final Tanh activation function to force all outputs to be in the range [-1, 1]\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        A function that generates fake samples for a batch of priors.\n",
    "\n",
    "        :param batch:   A batch of priors (random tensors).\n",
    "        :return:        A batch of fake samples.\n",
    "        \"\"\"\n",
    "        return self.sequential_model(batch)\n",
    "\n",
    "\n",
    "prior_size = 100\n",
    "dropout = 0.1\n",
    "leaky_relu_slope = 0.1\n",
    "generator = Generator(prior_size, feature_dim, dropout, leaky_relu_slope)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2 Defining the Discriminator\n",
    "\n",
    "The discriminator network takes as input a 1D tensor that could either be a real MNIST image or a fake image produced by the generator. It outputs a single scalar between 0 and 1, representing its confidence that the image is real. The discriminator network also typically includes several fully connected layers. We'll use the LeakyReLU activation function for these layers, and the Sigmoid function for the last layer to output a probability."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic implementation for a discriminator network.\n",
    "\n",
    "    This discriminator implementation uses a standard multi-layer perceptron (MLP).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size: int,\n",
    "            dropout: float,\n",
    "            leaky_relu_slope: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The initialization function for the discriminator.\n",
    "\n",
    "        :param input_size:              The input size of the discriminator.\n",
    "                                            This should be equal to the size of the feature space.\n",
    "        :param dropout:                 The dropout rate.\n",
    "        :param leaky_relu_slope:        The slope of the leaky ReLU activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the sequential multi-layer perceptron model using the given parameters.\n",
    "        self.sequential_model = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=1024),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.LeakyReLU(negative_slope=leaky_relu_slope, inplace=True),\n",
    "\n",
    "            nn.Linear(in_features=256, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        A function that computes the output of the discriminator for one batch of data.\n",
    "\n",
    "        :param batch:       The batch (2d Tensor) to compute the output for\n",
    "        :return:            The output (2d Tensor)\n",
    "        \"\"\"\n",
    "        return torch.sigmoid(self.sequential_model(batch))\n",
    "\n",
    "\n",
    "discriminator = Discriminator(feature_dim, dropout, leaky_relu_slope)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Training the GAN <a class='anchor' id='section6'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. Limitations of GANs <a class='anchor' id='section7'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8. Summary and Conclusion <a class='anchor' id='section8'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9. Exercises and Challenges <a class='anchor' id='section9'></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
